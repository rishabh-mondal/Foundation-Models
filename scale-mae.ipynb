{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae43abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.models._api import WeightsEnum\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torchgeo.models import scalemae_large_patch16, ScaleMAELarge16_Weights\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660bf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ScaleMAEBackbone(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Wraps TorchGeo ScaleMAE as a single-scale detection backbone for torchvision FasterRCNN.\n",
    "\n",
    "    Assumptions:\n",
    "      - ViT patch size = 16 (true for scalemae_large_patch16)\n",
    "      - model.forward_features(x) returns tokens or an embedding with a CLS token first\n",
    "      - We reshape tokens to (B, C, H', W') where H'=W'=img_size//16\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, weights=ScaleMAELarge16_Weights.FMOW_RGB, out_channels=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch = 16\n",
    "        assert img_size % self.patch == 0, \"img_size must be a multiple of patch size (16).\"\n",
    "        self.grid = img_size // self.patch  # e.g., 8 for 128x128\n",
    "\n",
    "        # Load pretrained ScaleMAE model; num_classes=0 to avoid creating a classifier head\n",
    "        self.body = scalemae_large_patch16(weights=weights)\n",
    "\n",
    "        # Figure out the embedding dimension from the model head or norm\n",
    "        # Commonly ViT-L/16 has embed_dim=1024, but we derive it programmatically.\n",
    "        # Try a forward pass to infer embed dim robustly.\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, img_size, img_size)\n",
    "            feats = self._forward_tokens(dummy)  # (1, grid*grid, embed_dim)\n",
    "            embed_dim = feats.shape[-1]\n",
    "\n",
    "        # Project to the out_channels expected by detection heads\n",
    "        self.proj = nn.Conv2d(embed_dim, out_channels, kernel_size=1)\n",
    "        self.out_channels = out_channels  # torchvision convention\n",
    "\n",
    "    def _forward_tokens(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns token sequence of shape (B, N, C). Removes CLS token if present.\n",
    "        We try the common ViT/Timm-style APIs.\n",
    "        \"\"\"\n",
    "        # 1) Preferred: forward_features\n",
    "        if hasattr(self.body, \"forward_features\"):\n",
    "            t = self.body.forward_features(x)  # could be (B, N, C) or dict\n",
    "            if isinstance(t, dict):\n",
    "                # If a dict (rare), try a common key\n",
    "                if \"x\" in t:\n",
    "                    t = t[\"x\"]\n",
    "                elif \"tokens\" in t:\n",
    "                    t = t[\"tokens\"]\n",
    "                else:\n",
    "                    # fall back: try to find the first tensor\n",
    "                    t = next(v for v in t.values() if torch.is_tensor(v))\n",
    "        else:\n",
    "            # 2) Fallback: call the model to get logits, then look for intermediate attr — not ideal.\n",
    "            # We strongly prefer forward_features, but keep a fallback that raises a clear error.\n",
    "            raise RuntimeError(\"ScaleMAE model does not expose forward_features; cannot extract tokens.\")\n",
    "\n",
    "        # t is now expected to be (B, N, C) with optional CLS token at t[:,0]\n",
    "        if t.dim() != 3:\n",
    "            # Some models return (B, C, H, W) already\n",
    "            if t.dim() == 4:\n",
    "                B, C, H, W = t.shape\n",
    "                return t.permute(0, 2, 3, 1).reshape(B, H*W, C)  # -> (B, N, C)\n",
    "            raise RuntimeError(f\"Unexpected feature shape from forward_features: {tuple(t.shape)}\")\n",
    "\n",
    "        # If there is a CLS token, N should be grid*grid + 1\n",
    "        B, N, C = t.shape\n",
    "        expected_N_with_cls = self.grid * self.grid + 1\n",
    "        expected_N_no_cls   = self.grid * self.grid\n",
    "        if N == expected_N_with_cls:\n",
    "            t = t[:, 1:, :]  # drop CLS\n",
    "        elif N != expected_N_no_cls:\n",
    "            # Some models may use different token layouts; try to reshape anyway and error if impossible\n",
    "            if N != expected_N_no_cls:\n",
    "                raise RuntimeError(\n",
    "                    f\"Token count {N} doesn't match expected {expected_N_no_cls} or {expected_N_with_cls} \"\n",
    "                    f\"for grid={self.grid} (img={self.img_size}, patch={self.patch}).\"\n",
    "                )\n",
    "        return t  # (B, grid*grid, C)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns a dict of feature maps. torchvision's FasterRCNN will use keys in insertion order.\n",
    "        \"\"\"\n",
    "        t = self._forward_tokens(x)  # (B, grid*grid, C)\n",
    "        B, N, C = t.shape\n",
    "        H = W = self.grid\n",
    "        fmap = t.transpose(1, 2).reshape(B, C, H, W)  # (B, C, H, W)\n",
    "        fmap = self.proj(fmap)  # (B, out_channels, H, W)\n",
    "        return {\"0\": fmap}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6db283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 304,912,104\n",
      "Trainable parameters:     304,912,104\n",
      "Non-trainable parameters: 201,728\n",
      "Total parameters:         305,113,832\n"
     ]
    }
   ],
   "source": [
    "backbone = ScaleMAEBackbone(weights=ScaleMAELarge16_Weights.FMOW_RGB, out_channels=768)\n",
    "total_params = sum(p.numel() for p in backbone.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "trainable_params = sum(p.numel() for p in backbone.parameters() if p.requires_grad)\n",
    "non_trainable_params = sum(p.numel() for p in backbone.parameters() if not p.requires_grad)\n",
    "\n",
    "print(f\"Trainable parameters:     {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "print(f\"Total parameters:         {trainable_params + non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef33847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scalemae_fasterrcnn(num_classes: int,\n",
    "                              img_size: int = 224,\n",
    "                              weights=ScaleMAELarge16_Weights.FMOW_RGB):\n",
    "    \"\"\"\n",
    "    Constructs a FasterRCNN detector that uses ScaleMAE as backbone.\n",
    "    \"\"\"\n",
    "    backbone = ScaleMAEBackbone(img_size=img_size, weights=weights, out_channels=768)\n",
    "\n",
    "    # Anchors: stride is 16 (ViT patch size). With 8×8 fmap at 128 input,\n",
    "    # choose relatively small anchor sizes. You can tune these based on object scale.\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((16, 24, 32, 48, 64),),       # pixels at input scale\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "\n",
    "    # ROI heads params can be tweaked; defaults are fine to start\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone,\n",
    "        num_classes=num_classes,    # include background as class 0 internally\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        min_size=224, \n",
    "        max_size=224,\n",
    "        box_detections_per_img=300\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb763acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully with ScaleMAE backbone.\n",
      "model footprint FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(224,), max_size=224, mode='bilinear')\n",
      "  )\n",
      "  (backbone): ScaleMAEBackbone(\n",
      "    (body): ScaleMAE(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "        (norm): Identity()\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (patch_drop): Identity()\n",
      "      (norm_pre): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (2): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (3): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (4): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (5): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (6): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (7): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (8): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (9): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (10): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (11): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (12): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (13): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (14): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (15): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (16): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (17): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (18): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (19): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (20): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (21): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (22): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (23): Block(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (fc_norm): Identity()\n",
      "      (head_drop): Dropout(p=0.0, inplace=False)\n",
      "      (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "    )\n",
      "    (proj): Conv2d(1024, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(768, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(768, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=37632, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=build_scalemae_fasterrcnn(num_classes=4)\n",
    "print(\"Model built successfully with ScaleMAE backbone.\")\n",
    "print(\"model footprint\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9ad5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 16\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "EPOCHS = 10\n",
    "CLASS_AGNOSTIC_EVAL = True  # set False for per-class AP\n",
    "\n",
    "# Paths\n",
    "TRAIN_ROOT = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/train\"\n",
    "TEST_ROOT  = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/test\"\n",
    "\n",
    "# Classes (your labels are 0-based in files, we add +1 so background=0 internally)\n",
    "# Adjust if you have different mapping\n",
    "CLASSES = [\"CFCBK\", \"FCBK\", \"Zigzag\"]\n",
    "NUM_CLASSES = len(CLASSES) + 1  # + background\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Dataset\n",
    "# -----------------------\n",
    "class BrickKilnDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects:\n",
    "      <root>/<split>/images/*.png\n",
    "      <root>/<split>/labels/*.txt\n",
    "    or if split is '' (empty), then <root>/images, <root>/labels\n",
    "\n",
    "    Label format assumed YOLO-OBB-like with 9 tokens per line (class + 8 coords normalized 0..1).\n",
    "    We convert OBB -> AABB (xyxy) after resizing to IMG_SIZE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: str, split: str, input_size: int = IMG_SIZE,\n",
    "                 mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.img_dir = self.root / split / 'images' if split else self.root / 'images'\n",
    "        self.label_dir = self.root / split / 'yolo_obb_labels' if split else self.root / 'yolo_obb_labels'\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "\n",
    "        self.img_files = []\n",
    "        all_files = sorted(os.listdir(self.img_dir))\n",
    "        logging.info(f\"Scanning {len(all_files)} images in {self.img_dir} for valid annotations...\")\n",
    "        for img_name in all_files:\n",
    "            if img_name.lower().endswith('.png') and self._has_valid_annotations(img_name):\n",
    "                self.img_files.append(img_name)\n",
    "        logging.info(f\"Found {len(self.img_files)} images with valid annotations in {self.img_dir}\")\n",
    "\n",
    "    def _has_valid_annotations(self, img_name: str) -> bool:\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "        if not label_path.exists():\n",
    "            return False\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if len(line.strip().split()) == 9:\n",
    "                        return True\n",
    "        except Exception:\n",
    "            return False\n",
    "        return False\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = self.img_dir / img_name\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_tensor = self.transform(img)  # (3, H, W)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error opening image {img_path}: {e}\")\n",
    "            # Return an empty target so collate can drop it\n",
    "            return torch.zeros(3, IMG_SIZE, IMG_SIZE), {\n",
    "                \"boxes\": torch.empty((0, 4), dtype=torch.float32),\n",
    "                \"labels\": torch.empty((0,), dtype=torch.int64),\n",
    "            }\n",
    "\n",
    "        _, new_h, new_w = img_tensor.shape\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 9:\n",
    "                        continue\n",
    "                    class_id = int(parts[0]) + 1  # shift by +1, background=0\n",
    "                    obb = np.array([float(p) for p in parts[1:]])\n",
    "                    x_coords = obb[0::2] * new_w\n",
    "                    y_coords = obb[1::2] * new_h\n",
    "                    xmin, ymin = float(np.min(x_coords)), float(np.min(y_coords))\n",
    "                    xmax, ymax = float(np.max(x_coords)), float(np.max(y_coords))\n",
    "                    if xmax > xmin and ymax > ymin:\n",
    "                        boxes.append([xmin, ymin, xmax, ymax])\n",
    "                        labels.append(class_id)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "        }\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # drop images with no boxes\n",
    "    batch = [b for b in batch if b[1][\"boxes\"].numel() > 0]\n",
    "    if not batch:\n",
    "        return [], []\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e2a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_scalemae_fasterrcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52de7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes: int, img_size: int = IMG_SIZE):\n",
    "#     # Use ScaleMAE pretrained mean/std in the dataset; so set identity stats here\n",
    "#     weights = ScaleMAELarge16_Weights.FMOW_RGB\n",
    "#     backbone = ScaleMAEBackbone(img_size=img_size, weights=weights, out_channels=768)\n",
    "\n",
    "#     # Anchor sizes tuned to stride=16 & 224 input (feature map 14x14). Tweak as needed.\n",
    "#     anchor_generator = AnchorGenerator(sizes=((16, 24, 32, 48, 64),),\n",
    "#                                        aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "#     model = FasterRCNN(\n",
    "#         backbone=backbone,\n",
    "#         num_classes=num_classes,\n",
    "#         rpn_anchor_generator=anchor_generator,\n",
    "#         # Transform: keep 224x224 and identity normalization (dataset handles norm)\n",
    "#         min_size=img_size,\n",
    "#         max_size=img_size,\n",
    "#         image_mean=[0.0, 0.0, 0.0],\n",
    "#         image_std=[1.0, 1.0, 1.0],\n",
    "#         box_detections_per_img=300,\n",
    "#     )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3959639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model=build_model(num_classes=NUM_CLASSES, img_size=IMG_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d4db7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf820d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Training & Evaluation\n",
    "# -----------------------\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for images, targets in loader:\n",
    "        if len(images) == 0:\n",
    "            continue\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        losses = model(images, targets)\n",
    "        loss = sum(v for v in losses.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item()\n",
    "    return running / max(1, len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ff6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm\n",
    "import logging, numpy as np\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_report(model, data_loader, device, epoch=0, class_names=None, writer=None, desc_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      {\n",
    "        \"ca_ap50\": float,                    # class-agnostic AP@50\n",
    "        \"ca_ap5095\": float,                  # class-agnostic AP@[.50:.95]\n",
    "        \"cw_ap50\": dict[name_or_id -> ap],   # class-wise AP@50\n",
    "        \"cw_ap5095\": dict[name_or_id -> ap], # class-wise AP@[.50:.95]\n",
    "      }\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loop = tqdm(data_loader, desc=f\"Epoch {epoch} [Validation{desc_suffix}]\")\n",
    "\n",
    "    # Class-aware (per-class metrics across .50:.95)\n",
    "    tm_cls = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True)\n",
    "    # Class-agnostic (collapse all labels)\n",
    "    tm_agn = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=False)\n",
    "\n",
    "    for images, targets in loop:\n",
    "        if not images: \n",
    "            continue\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Move to CPU for TM\n",
    "        preds = [{k: v.detach().cpu() for k, v in o.items()} for o in outputs]\n",
    "        tgts  = [{k: v.detach().cpu() for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Class-aware update\n",
    "        try:\n",
    "            tm_cls.update(preds, tgts)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[validate_report] class-aware update failed: {e}\")\n",
    "\n",
    "        # Class-agnostic copies (labels -> 0)\n",
    "        preds_agn = [{\"boxes\": d[\"boxes\"], \"scores\": d[\"scores\"], \"labels\": d[\"labels\"].new_zeros(d[\"labels\"].shape)} for d in preds]\n",
    "        tgts_agn  = [{\"boxes\": d[\"boxes\"], \"labels\": d[\"labels\"].new_zeros(d[\"labels\"].shape)} for d in tgts]\n",
    "        try:\n",
    "            tm_agn.update(preds_agn, tgts_agn)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[validate_report] class-agnostic update failed: {e}\")\n",
    "\n",
    "    # Compute\n",
    "    res_cls = tm_cls.compute()\n",
    "    res_agn = tm_agn.compute()\n",
    "\n",
    "    # ---- (1) CA AP@50 and (2) CA AP@50:95 ----\n",
    "    ca_ap50   = float(res_agn[\"map_50\"].item()) if res_agn[\"map_50\"].numel() else 0.0\n",
    "    ca_ap5095 = float(res_agn[\"map\"].item())    if res_agn[\"map\"].numel()    else 0.0\n",
    "\n",
    "    # ---- (3) Class-wise AP@50 ----\n",
    "    cw_ap50 = {}\n",
    "    if res_cls.get(\"map_50_per_class\") is not None:\n",
    "        vals = res_cls[\"map_50_per_class\"].tolist()\n",
    "        classes = res_cls.get(\"classes\")\n",
    "        if classes is not None and classes.numel() == len(vals):\n",
    "            for cid, m in zip(classes.tolist(), vals):\n",
    "                if cid == 0:  # skip background if present\n",
    "                    continue\n",
    "                name = class_names[cid-1] if class_names and (cid-1) < len(class_names) else f\"class_{cid}\"\n",
    "                cw_ap50[name] = float(m) if np.isfinite(m) else float(\"nan\")\n",
    "        else:\n",
    "            for i, m in enumerate(vals, start=1):\n",
    "                name = class_names[i-1] if class_names and (i-1) < len(class_names) else f\"class_{i}\"\n",
    "                cw_ap50[name] = float(m) if np.isfinite(m) else float(\"nan\")\n",
    "\n",
    "    # ---- (4) Class-wise AP@50:95 ----\n",
    "    cw_ap5095 = {}\n",
    "    if res_cls.get(\"map_per_class\") is not None:\n",
    "        vals = res_cls[\"map_per_class\"].tolist()\n",
    "        classes = res_cls.get(\"classes\")\n",
    "        if classes is not None and classes.numel() == len(vals):\n",
    "            for cid, m in zip(classes.tolist(), vals):\n",
    "                if cid == 0:\n",
    "                    continue\n",
    "                name = class_names[cid-1] if class_names and (cid-1) < len(class_names) else f\"class_{cid}\"\n",
    "                cw_ap5095[name] = float(m) if np.isfinite(m) else float(\"nan\")\n",
    "        else:\n",
    "            for i, m in enumerate(vals, start=1):\n",
    "                name = class_names[i-1] if class_names and (i-1) < len(class_names) else f\"class_{i}\"\n",
    "                cw_ap5095[name] = float(m) if np.isfinite(m) else float(\"nan\")\n",
    "\n",
    "    # Logging / TensorBoard\n",
    "    logging.info(\n",
    "        f\"CA AP@50: {ca_ap50:.4f} | CA AP@[.50:.95]: {ca_ap5095:.4f} | \"\n",
    "        f\"CW AP@50: {cw_ap50} | CW AP@[.50:.95]: {cw_ap5095}\"\n",
    "    )\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(f\"mAP/ca_ap50{desc_suffix}\", ca_ap50, epoch)\n",
    "        writer.add_scalar(f\"mAP/ca_ap5095{desc_suffix}\", ca_ap5095, epoch)\n",
    "        for k, v in cw_ap50.items():\n",
    "            writer.add_scalar(f\"mAP/cw_ap50_{k}{desc_suffix}\", v, epoch)\n",
    "        for k, v in cw_ap5095.items():\n",
    "            writer.add_scalar(f\"mAP/cw_ap5095_{k}{desc_suffix}\", v, epoch)\n",
    "\n",
    "    return {\n",
    "        \"ca_ap50\": ca_ap50,\n",
    "        \"ca_ap5095\": ca_ap5095,\n",
    "        \"cw_ap50\": cw_ap50,\n",
    "        \"cw_ap5095\": cw_ap5095,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def param_fingerprint(model) -> dict:\n",
    "    \"\"\"Return quick fingerprints of current model params.\"\"\"\n",
    "    # sum of L2 norms (fast), and a tiny SHA256 over a small sample (cheap)\n",
    "    l2_sum = 0.0\n",
    "    hasher = hashlib.sha256()\n",
    "    with torch.no_grad():\n",
    "        for i, p in enumerate(model.parameters()):\n",
    "            if p.requires_grad and p.numel() > 0:\n",
    "                l2_sum += float(torch.linalg.vector_norm(p.detach()).cpu())\n",
    "                # sample first 1024 elements to keep it cheap\n",
    "                s = p.detach().view(-1).float().cpu()\n",
    "                hasher.update(s[: min(1024, s.numel())].numpy().tobytes())\n",
    "    return {\"l2\": l2_sum, \"sha\": hasher.hexdigest()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6889dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    train_ds = BrickKilnDataset(TRAIN_ROOT, split=\"\", input_size=IMG_SIZE)\n",
    "    test_ds  = BrickKilnDataset(TEST_ROOT,  split=\"\", input_size=IMG_SIZE)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "    model = build_scalemae_fasterrcnn(NUM_CLASSES, img_size=IMG_SIZE).to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # (optional) baseline eval BEFORE any training\n",
    "    base_fp = param_fingerprint(model)\n",
    "    logging.info(f\"[E0/before-train] fp: {base_fp}\")\n",
    "\n",
    "    best_ca_ap50 = 0.0\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1), desc=\"Training\"):\n",
    "        # ---- fingerpint BEFORE training this epoch\n",
    "        fp_before = param_fingerprint(model)\n",
    "        logging.info(f\"[E{epoch}/pre-train] fp: {fp_before}\")\n",
    "\n",
    "        # ---- train (this should call optimizer.step() internally)\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "\n",
    "        # ---- fingerprint AFTER training this epoch\n",
    "        fp_after = param_fingerprint(model)\n",
    "        logging.info(f\"[E{epoch}/post-train] fp: {fp_after}\")\n",
    "\n",
    "        # Sanity: did weights change?\n",
    "        if abs(fp_after[\"l2\"] - fp_before[\"l2\"]) < 1e-6 or fp_after[\"sha\"] == fp_before[\"sha\"]:\n",
    "            logging.warning(f\"[E{epoch}] WARNING: weights fingerprint unchanged after training. \"\n",
    "                            \"Check that gradients are non-zero, the LR isn’t zero, \"\n",
    "                            \"and optimizer.step() is executed.\")\n",
    "\n",
    "        # ---- right BEFORE validation (should match post-train)\n",
    "        fp_preval = param_fingerprint(model)\n",
    "        if fp_preval[\"sha\"] != fp_after[\"sha\"]:\n",
    "            logging.warning(f\"[E{epoch}] Model changed between post-train and pre-val (unexpected).\")\n",
    "\n",
    "        # ---- Evaluate (CA AP@50, CA AP@50:95, CW AP@50, CW AP@50:95)\n",
    "        metrics = validate_report(model, test_loader, device, epoch, class_names=CLASSES, writer=None)\n",
    "        ca_ap50   = metrics[\"ca_ap50\"]\n",
    "        ca_ap5095 = metrics[\"ca_ap5095\"]\n",
    "        cw_ap50   = metrics[\"cw_ap50\"]\n",
    "        cw_ap5095 = metrics[\"cw_ap5095\"]\n",
    "\n",
    "        cw50_str   = \" | \".join(f\"{k}: {v:.4f}\" for k, v in cw_ap50.items())\n",
    "        cw5095_str = \" | \".join(f\"{k}: {v:.4f}\" for k, v in cw_ap5095.items())\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | loss={train_loss:.4f} | \"\n",
    "            f\"CA AP@50={ca_ap50:.4f} | CA AP@[.50:.95]={ca_ap5095:.4f}\\n\"\n",
    "            f\"  CW AP@50      -> {cw50_str}\\n\"\n",
    "            f\"  CW AP@[.50:.95]-> {cw5095_str}\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"[E{epoch}] CA AP@50={ca_ap50:.4f} | CA AP@[.50:.95]={ca_ap5095:.4f} | \"\n",
    "            f\"CW AP@50={cw_ap50} | CW AP@[.50:.95]={cw_ap5095}\"\n",
    "        )\n",
    "\n",
    "        # Save best by CA AP@50\n",
    "        if ca_ap50 > best_ca_ap50:\n",
    "            best_ca_ap50 = ca_ap50\n",
    "            ckpt = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"metrics\": metrics,\n",
    "                \"fp_post_train\": fp_after,  # store the fingerprint used for this eval\n",
    "                \"cfg\": {\n",
    "                    \"IMG_SIZE\": IMG_SIZE,\n",
    "                    \"CLASSES\": CLASSES,\n",
    "                    \"NUM_CLASSES\": NUM_CLASSES,\n",
    "                    \"LR\": LR,\n",
    "                    \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "                }\n",
    "            }\n",
    "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "            torch.save(ckpt, \"checkpoints/scalemae_frcnn_best.pth\")\n",
    "            logging.info(f\"Saved best checkpoint with CA AP@50={ca_ap50:.4f}\")\n",
    "\n",
    "    logging.info(f\"Training done. Best CA AP@50={best_ca_ap50:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61bef162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scanning 47214 images in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/train/images for valid annotations...\n",
      "INFO:root:Found 47214 images with valid annotations in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/train/images\n",
      "INFO:root:Scanning 15738 images in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/test/images for valid annotations...\n",
      "INFO:root:Found 15738 images with valid annotations in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/test/images\n",
      "INFO:root:[E0/before-train] fp: {'l2': 9875.787951219827, 'sha': '6288aadfc651b46fc0a87e01d620da2acfe5904295b0317a988819ff6bd486c2'}\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]INFO:root:[E1/pre-train] fp: {'l2': 9875.787951219827, 'sha': '6288aadfc651b46fc0a87e01d620da2acfe5904295b0317a988819ff6bd486c2'}\n",
      "INFO:root:[E1/post-train] fp: {'l2': 9865.51096666744, 'sha': '73f94d2ae0e788ea11d09030feaba7f4b0ce454934aa9599126579ec3d52f88f'}\n",
      "Epoch 1 [Validation]: 100%|██████████| 123/123 [03:36<00:00,  1.76s/it]\n",
      "INFO:root:CA AP@50: 0.0000 | CA AP@[.50:.95]: 0.0000 | CW AP@50: {} | CW AP@[.50:.95]: {'CFCBK': 0.0, 'FCBK': 0.0, 'Zigzag': 0.0}\n",
      "INFO:root:[E1] CA AP@50=0.0000 | CA AP@[.50:.95]=0.0000 | CW AP@50={} | CW AP@[.50:.95]={'CFCBK': 0.0, 'FCBK': 0.0, 'Zigzag': 0.0}\n",
      "Training:  10%|█         | 1/10 [28:58<4:20:49, 1738.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.4202 | CA AP@50=0.0000 | CA AP@[.50:.95]=0.0000\n",
      "  CW AP@50      -> \n",
      "  CW AP@[.50:.95]-> CFCBK: 0.0000 | FCBK: 0.0000 | Zigzag: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[E2/pre-train] fp: {'l2': 9865.51096666744, 'sha': '73f94d2ae0e788ea11d09030feaba7f4b0ce454934aa9599126579ec3d52f88f'}\n",
      "INFO:root:[E2/post-train] fp: {'l2': 9855.05748561304, 'sha': '13c3d0348a3241bbe615c1305ea87a031ff296ba495d97e762474638a5e7e93f'}\n",
      "Epoch 2 [Validation]: 100%|██████████| 123/123 [03:31<00:00,  1.72s/it]\n",
      "INFO:root:CA AP@50: 0.0000 | CA AP@[.50:.95]: 0.0000 | CW AP@50: {} | CW AP@[.50:.95]: {'CFCBK': 0.0, 'FCBK': 0.0, 'Zigzag': 0.0}\n",
      "INFO:root:[E2] CA AP@50=0.0000 | CA AP@[.50:.95]=0.0000 | CW AP@50={} | CW AP@[.50:.95]={'CFCBK': 0.0, 'FCBK': 0.0, 'Zigzag': 0.0}\n",
      "Training:  20%|██        | 2/10 [57:51<3:51:21, 1735.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss=0.2715 | CA AP@50=0.0000 | CA AP@[.50:.95]=0.0000\n",
      "  CW AP@50      -> \n",
      "  CW AP@[.50:.95]-> CFCBK: 0.0000 | FCBK: 0.0000 | Zigzag: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[E3/pre-train] fp: {'l2': 9855.05748561304, 'sha': '13c3d0348a3241bbe615c1305ea87a031ff296ba495d97e762474638a5e7e93f'}\n",
      "INFO:root:[E3/post-train] fp: {'l2': 9849.297889551148, 'sha': '814844b59e71aea46adb34496cdac018d2cd2b1fc0f61db31d855b0424dd07ef'}\n",
      "Epoch 3 [Validation]: 100%|██████████| 123/123 [03:38<00:00,  1.78s/it]\n",
      "INFO:root:CA AP@50: 0.0000 | CA AP@[.50:.95]: 0.0000 | CW AP@50: {} | CW AP@[.50:.95]: {'CFCBK': 0.0, 'FCBK': 0.0, 'Zigzag': 0.0}\n",
      "INFO:root:[E3] CA AP@50=0.0000 | CA AP@[.50:.95]=0.0000 | CW AP@50={} | CW AP@[.50:.95]={'CFCBK': 0.0, 'FCBK': 0.0, 'Zigzag': 0.0}\n",
      "Training:  30%|███       | 3/10 [1:26:49<3:22:34, 1736.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | loss=0.2058 | CA AP@50=0.0000 | CA AP@[.50:.95]=0.0000\n",
      "  CW AP@50      -> \n",
      "  CW AP@[.50:.95]-> CFCBK: 0.0000 | FCBK: 0.0000 | Zigzag: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[E4/pre-train] fp: {'l2': 9849.297889551148, 'sha': '814844b59e71aea46adb34496cdac018d2cd2b1fc0f61db31d855b0424dd07ef'}\n",
      "Training:  30%|███       | 3/10 [1:29:02<3:27:46, 1780.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[E\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/pre-train] fp: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfp_before\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# ---- train (this should call optimizer.step() internally)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ---- fingerprint AFTER training this epoch\u001b[39;00m\n\u001b[1;32m     31\u001b[0m fp_after \u001b[38;5;241m=\u001b[39m param_fingerprint(model)\n",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m     running \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(loader))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2c023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99414865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
