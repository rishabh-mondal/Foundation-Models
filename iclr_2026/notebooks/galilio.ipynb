{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca29bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# --- TorchVision & TorchMetrics Imports ---\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25c0747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/iclr_2026/models', '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/iclr_2026/models', '/opt/anaconda3/envs/rishabh_sat/lib/python312.zip', '/opt/anaconda3/envs/rishabh_sat/lib/python3.12', '/opt/anaconda3/envs/rishabh_sat/lib/python3.12/lib-dynload', '', '/home/rishabh.mondal/.local/lib/python3.12/site-packages', '/opt/anaconda3/envs/rishabh_sat/lib/python3.12/site-packages', '/opt/anaconda3/envs/rishabh_sat/lib/python3.12/site-packages/ISR-2.2.0-py3.12.egg', '/opt/anaconda3/envs/rishabh_sat/lib/python3.12/site-packages/setuptools/_vendor', '/home/rishabh.mondal/solo-learn', '/tmp/tmpb_rwcmur']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Add Galileo to Path and Import ---\n",
    "sys.path.insert(0, str(Path.cwd() / \"models\"))\n",
    "print(sys.path)\n",
    "try:\n",
    "    from galileo import (\n",
    "        Encoder as GalileoEncoder,\n",
    "        SPACE_TIME_BANDS, SPACE_TIME_BANDS_GROUPS_IDX,\n",
    "        SPACE_BANDS, SPACE_BAND_GROUPS_IDX,\n",
    "        TIME_BANDS, TIME_BAND_GROUPS_IDX,\n",
    "        STATIC_BANDS, STATIC_BAND_GROUPS_IDX\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"Error: Could not import Galileo model. Make sure 'models/galileo.py' exists.\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "764849ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalileoBackboneWrapper(nn.Module):\n",
    "    def __init__(self, pretrained_path: str, patch_size: int = 8):\n",
    "        super().__init__()\n",
    "        logging.info(f\"Loading Galileo encoder from {pretrained_path}...\")\n",
    "        self.encoder = GalileoEncoder.load_from_folder(Path(pretrained_path), device='cpu')\n",
    "        self.out_channels = self.encoder.embedding_size\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(self.encoder.embedding_size, self.out_channels, kernel_size=1)\n",
    "        logging.info(f\"Galileo backbone initialized with output channels: {self.out_channels}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> OrderedDict:\n",
    "        print(f\"[Backbone] Input x: {x.shape}\")  # <-- print input to backbone\n",
    "\n",
    "        b, _, h, w = x.shape\n",
    "        s_t_x = torch.zeros(b, h, w, 1, len(SPACE_TIME_BANDS), device=x.device, dtype=x.dtype)\n",
    "        s2_rgb_indices = [SPACE_TIME_BANDS.index(b) for b in [\"B2\", \"B3\", \"B4\"]]\n",
    "        s_t_x[..., s2_rgb_indices] = x.permute(0, 2, 3, 1).unsqueeze(-2)\n",
    "        print(f\"[Backbone] s_t_x: {s_t_x.shape}\")\n",
    "\n",
    "        s_t_m = torch.ones(b, h, w, 1, len(SPACE_TIME_BANDS_GROUPS_IDX), device=x.device, dtype=torch.long)\n",
    "        s2_rgb_group_idx = list(SPACE_TIME_BANDS_GROUPS_IDX.keys()).index('S2_RGB')\n",
    "        s_t_m[..., s2_rgb_group_idx] = 0\n",
    "        print(f\"[Backbone] s_t_m: {s_t_m.shape}\")\n",
    "\n",
    "        data_args, mask_args = {'device': x.device, 'dtype': x.dtype}, {'device': x.device, 'dtype': torch.long}\n",
    "        sp_x, t_x, st_x = torch.zeros(b, h, w, len(SPACE_BANDS), **data_args), torch.zeros(b, 1, len(TIME_BANDS), **data_args), torch.zeros(b, len(STATIC_BANDS), **data_args)\n",
    "        sp_m, t_m, st_m = torch.ones(b, h, w, len(SPACE_BAND_GROUPS_IDX), **mask_args), torch.ones(b, 1, len(TIME_BAND_GROUPS_IDX), **mask_args), torch.ones(b, len(STATIC_BAND_GROUPS_IDX), **mask_args)\n",
    "        months = torch.ones(b, 1, device=x.device, dtype=torch.long) * 6\n",
    "\n",
    "        s_t_out, _, _, _, _, _, _, _, _ = self.encoder(\n",
    "            s_t_x, sp_x, t_x, st_x, s_t_m, sp_m, t_m, st_m, months,\n",
    "            patch_size=self.patch_size, add_layernorm_on_exit=True\n",
    "        )\n",
    "        print(f\"[Backbone] s_t_out: {s_t_out.shape}\")\n",
    "\n",
    "        feature_tokens = s_t_out[:, :, :, 0, s2_rgb_group_idx, :]\n",
    "        print(f\"[Backbone] feature_tokens: {feature_tokens.shape}\")\n",
    "\n",
    "        feature_map = feature_tokens.permute(0, 3, 1, 2).contiguous()\n",
    "        print(f\"[Backbone] feature_map: {feature_map.shape}\")\n",
    "\n",
    "        projected_map = self.projection(feature_map)\n",
    "        print(f\"[Backbone] projected_map: {projected_map.shape}\")\n",
    "\n",
    "        return OrderedDict([(\"0\", projected_map)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0e87028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(weights_path: str, num_classes: int) -> nn.Module:\n",
    "    backbone = GalileoBackboneWrapper(pretrained_path=weights_path)\n",
    "    anchor_sizes = ((16, 32, 64, 96, 128),)\n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "    anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
    "    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=anchor_generator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99cb496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Backbone] Input x: torch.Size([1, 3, 800, 800])\n",
      "[Backbone] s_t_x: torch.Size([1, 800, 800, 1, 13])\n",
      "[Backbone] s_t_m: torch.Size([1, 800, 800, 1, 7])\n",
      "[Backbone] s_t_out: torch.Size([1, 100, 100, 1, 7, 128])\n",
      "[Backbone] feature_tokens: torch.Size([1, 100, 100, 128])\n",
      "[Backbone] feature_map: torch.Size([1, 128, 100, 100])\n",
      "[Backbone] projected_map: torch.Size([1, 128, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(1, 3, 96, 96)\n",
    "model = create_model(weights_path=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/iclr_2026/nano\", num_classes=4)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(dummy)\n",
    "# print(\"Final output:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "649aa3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape_hook(module, input, output):\n",
    "    name = module.__class__.__name__\n",
    "    if isinstance(output, (list, tuple)):\n",
    "        shapes = [o.shape if hasattr(o, \"shape\") else type(o) for o in output]\n",
    "    else:\n",
    "        shapes = output.shape if hasattr(output, \"shape\") else type(output)\n",
    "    print(f\"[{name}] output shape: {shapes}\")\n",
    "\n",
    "# Attach hooks to key parts of FasterRCNN\n",
    "for name, module in model.named_modules():\n",
    "    if any(k in name.lower() for k in [\"rpn\", \"roi_heads\", \"box_head\", \"box_predictor\"]):\n",
    "        module.register_forward_hook(print_shape_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9eac165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): GalileoBackboneWrapper(\n",
      "    (encoder): Encoder(\n",
      "      (blocks): ModuleListWithInit(\n",
      "        (0-3): 4 x Block(\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (k): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (v): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (month_embed): Embedding(12, 32)\n",
      "      (space_time_embed): ModuleDict(\n",
      "        (S1): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(2, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (S2_RGB): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(3, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (S2_Red_Edge): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(3, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (S2_NIR_10m): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(1, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (S2_NIR_20m): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(1, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (S2_SWIR): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(2, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (NDVI): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(1, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "      )\n",
      "      (space_embed): ModuleDict(\n",
      "        (SRTM): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(2, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (DW): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(9, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (WC): FlexiPatchEmbed(\n",
      "          (proj): Conv2d(5, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "      )\n",
      "      (time_embed): ModuleDict(\n",
      "        (ERA5): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (TC): Linear(in_features=3, out_features=128, bias=True)\n",
      "        (VIIRS): Linear(in_features=1, out_features=128, bias=True)\n",
      "      )\n",
      "      (static_embed): ModuleDict(\n",
      "        (LS): Linear(in_features=1, out_features=128, bias=True)\n",
      "        (location): Linear(in_features=3, out_features=128, bias=True)\n",
      "        (DW_static): Linear(in_features=9, out_features=128, bias=True)\n",
      "        (WC_static): Linear(in_features=5, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (projection): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(128, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(128, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=6272, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fbe15f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Backbone] Input x: (1, 3, 128, 128)\n",
      "[Backbone] s_t_x: (1, 128, 128, 1, 13)\n",
      "[Backbone] s_t_m: (1, 128, 128, 1, 7)\n",
      "[Backbone] s_t_out: (1, 16, 16, 1, 7, 128)\n",
      "[Backbone] feature_tokens: (1, 16, 16, 128)\n",
      "[Backbone] feature_map: (1, 128, 16, 16)\n",
      "[Backbone] projected_map: (1, 128, 16, 16)\n",
      "OK. Forward pass complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "# --- Your Galileo imports must already work ---\n",
    "# from galileo import (\n",
    "#     Encoder as GalileoEncoder,\n",
    "#     SPACE_TIME_BANDS, SPACE_TIME_BANDS_GROUPS_IDX,\n",
    "#     SPACE_BANDS, SPACE_BAND_GROUPS_IDX,\n",
    "#     TIME_BANDS, TIME_BAND_GROUPS_IDX,\n",
    "#     STATIC_BANDS, STATIC_BAND_GROUPS_IDX\n",
    "# )\n",
    "\n",
    "# ===== Backbone (unchanged except for optional prints) =====\n",
    "class GalileoBackboneWrapper(nn.Module):\n",
    "    def __init__(self, pretrained_path: str, patch_size: int = 8, debug: bool = False):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        logging.info(f\"Loading Galileo encoder from {pretrained_path}...\")\n",
    "        self.encoder = GalileoEncoder.load_from_folder(Path(pretrained_path), device='cpu')\n",
    "        self.out_channels = self.encoder.embedding_size\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(self.encoder.embedding_size, self.out_channels, kernel_size=1)\n",
    "        logging.info(f\"Galileo backbone initialized with output channels: {self.out_channels}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> OrderedDict:\n",
    "        if self.debug: print(f\"[Backbone] Input x: {tuple(x.shape)}\")\n",
    "\n",
    "        b, _, h, w = x.shape\n",
    "        s_t_x = torch.zeros(b, h, w, 1, len(SPACE_TIME_BANDS), device=x.device, dtype=x.dtype)\n",
    "        s2_rgb_indices = [SPACE_TIME_BANDS.index(band) for band in [\"B2\", \"B3\", \"B4\"]]\n",
    "        s_t_x[..., s2_rgb_indices] = x.permute(0, 2, 3, 1).unsqueeze(-2)\n",
    "        if self.debug: print(f\"[Backbone] s_t_x: {tuple(s_t_x.shape)}\")\n",
    "\n",
    "        s_t_m = torch.ones(b, h, w, 1, len(SPACE_TIME_BANDS_GROUPS_IDX), device=x.device, dtype=torch.long)\n",
    "        s2_rgb_group_idx = list(SPACE_TIME_BANDS_GROUPS_IDX.keys()).index('S2_RGB')\n",
    "        s_t_m[..., s2_rgb_group_idx] = 0\n",
    "        if self.debug: print(f\"[Backbone] s_t_m: {tuple(s_t_m.shape)}\")\n",
    "\n",
    "        data_args, mask_args = {'device': x.device, 'dtype': x.dtype}, {'device': x.device, 'dtype': torch.long}\n",
    "        sp_x = torch.zeros(b, h, w, len(SPACE_BANDS), **data_args)\n",
    "        t_x  = torch.zeros(b, 1, len(TIME_BANDS), **data_args)\n",
    "        st_x = torch.zeros(b, len(STATIC_BANDS), **data_args)\n",
    "        sp_m = torch.ones(b, h, w, len(SPACE_BAND_GROUPS_IDX), **mask_args)\n",
    "        t_m  = torch.ones(b, 1, len(TIME_BAND_GROUPS_IDX), **mask_args)\n",
    "        st_m = torch.ones(b, len(STATIC_BAND_GROUPS_IDX), **mask_args)\n",
    "        months = torch.ones(b, 1, device=x.device, dtype=torch.long) * 6  # adjust if 0/1-indexed months differ\n",
    "\n",
    "        s_t_out, *_ = self.encoder(\n",
    "            s_t_x, sp_x, t_x, st_x, s_t_m, sp_m, t_m, st_m, months,\n",
    "            patch_size=self.patch_size, add_layernorm_on_exit=True\n",
    "        )\n",
    "        if self.debug: print(f\"[Backbone] s_t_out: {tuple(s_t_out.shape)}\")  # [B, H', W', 1, Groups, C]\n",
    "\n",
    "        feature_tokens = s_t_out[:, :, :, 0, s2_rgb_group_idx, :]  # [B, H', W', C]\n",
    "        if self.debug: print(f\"[Backbone] feature_tokens: {tuple(feature_tokens.shape)}\")\n",
    "\n",
    "        feature_map = feature_tokens.permute(0, 3, 1, 2).contiguous()  # [B, C, H', W']\n",
    "        if self.debug: print(f\"[Backbone] feature_map: {tuple(feature_map.shape)}\")\n",
    "\n",
    "        projected_map = self.projection(feature_map)  # [B, C(out), H', W']\n",
    "        if self.debug: print(f\"[Backbone] projected_map: {tuple(projected_map.shape)}\")\n",
    "\n",
    "        return OrderedDict([(\"0\", projected_map)])\n",
    "\n",
    "\n",
    "# ===== Patched create_model forcing 128×128 =====\n",
    "def create_model(\n",
    "    weights_path: str,\n",
    "    num_classes: int,\n",
    "    img_size: int = 128,\n",
    "    patch_size: int = 8,\n",
    "    debug_backbone: bool = False,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Builds Faster R-CNN with:\n",
    "      - GalileoBackboneWrapper\n",
    "      - Transform fixed to img_size x img_size (no auto-resize to 800)\n",
    "      - Anchors chosen for small images\n",
    "    \"\"\"\n",
    "    # Backbone\n",
    "    backbone = GalileoBackboneWrapper(pretrained_path=weights_path, patch_size=patch_size, debug=debug_backbone)\n",
    "\n",
    "    # Anchor sizes chosen for a 128×128 image with stride=patch_size (i.e., feature map ~16×16).\n",
    "    # Adjust if your objects are much smaller/larger.\n",
    "    anchor_sizes = ((16, 32, 64),)  # pixels on the input image\n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),)\n",
    "    anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
    "\n",
    "    # Build model\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        # box_nms_thresh, rpn_nms_thresh, etc. can be tuned later\n",
    "    )\n",
    "\n",
    "    # OVERRIDE the internal image transform to FIX input size to 128×128\n",
    "    # If your data are already in [0,1], using ImageNet stats is fine; adjust if using a different normalization.\n",
    "    model.transform = GeneralizedRCNNTransform(\n",
    "        min_size=img_size,   # shorter side\n",
    "        max_size=img_size,   # longer side\n",
    "        image_mean=[0.485, 0.456, 0.406],\n",
    "        image_std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ===== Quick smoke test (run this once to verify shapes) =====\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = create_model(\n",
    "        weights_path=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/iclr_2026/nano\",\n",
    "        num_classes=2,\n",
    "        img_size=128,\n",
    "        patch_size=8,\n",
    "        debug_backbone=True,   # set False to silence prints\n",
    "    ).to(device).eval()\n",
    "\n",
    "    dummy = torch.randn(1, 3, 128, 128, device=device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(dummy)   # will keep input at 128×128 internally\n",
    "    print(\"OK. Forward pass complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a890096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
